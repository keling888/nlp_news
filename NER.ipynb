{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_with_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'str'> <class 'float'>]\n"
     ]
    }
   ],
   "source": [
    "print(df['text'].apply(type).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with float values:\n",
      "       Unnamed: 0                                                url  \\\n",
      "29616       31770  https://www.mondaq.com:443/unitedstates/licens...   \n",
      "37172       39836  https://www.mondaq.com:443/canada/copyright/10...   \n",
      "59093       63285              https://file770.com/copyright-and-ai/   \n",
      "61721       66115  https://www.windowscentral.com/software-apps/a...   \n",
      "88360       94643  https://www.law360.com/articles/1697413/copyri...   \n",
      "\n",
      "             date language                                              title  \\\n",
      "29616  2024-02-01       en  Copyright Office Seeks Comments On Compulsory ...   \n",
      "37172  2021-08-05       en  Copyright Act Consultation To Address Artifici...   \n",
      "59093  2023-04-29       en                      Copyright and AI | File 770\\t   \n",
      "61721  2024-05-01       en  Copyright infringement continues to be a pain ...   \n",
      "88360  2023-07-10       en  Copyright Precautions For AI Content After War...   \n",
      "\n",
      "      text  sentiment_score sentiment_label  \n",
      "29616  NaN              0.0        Negative  \n",
      "37172  NaN              0.0        Negative  \n",
      "59093  NaN              0.0        Negative  \n",
      "61721  NaN              0.0        Negative  \n",
      "88360  NaN              0.0        Negative  \n"
     ]
    }
   ],
   "source": [
    "# Identify rows where the type is float\n",
    "float_rows = df[df['text'].apply(type) == float]\n",
    "\n",
    "# Print the rows with float data type\n",
    "print(\"Rows with float values:\")\n",
    "print(float_rows.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_pipelines(model=\"dslim/bert-base-NER\", batch_size=32):\n",
    "    \"\"\"Setup NER pipeline with GPU support\"\"\"\n",
    "    device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "    ner_pipeline = pipeline(\n",
    "        \"ner\",\n",
    "        model=model,\n",
    "        tokenizer=model,\n",
    "        device=device,  # Enable GPU\n",
    "        batch_size=batch_size,\n",
    "        aggregation_strategy=\"none\"\n",
    "    )\n",
    "    return ner_pipeline\n",
    "\n",
    "def process_batch(texts, ner_pipe, sentiment_scores, dates):\n",
    "    \"\"\"Process a batch of texts for entity extraction\"\"\"\n",
    "    entity_sentiments = defaultdict(list)\n",
    "    try:\n",
    "        # Get NER results for the batch\n",
    "        ner_results = ner_pipe(texts)\n",
    "\n",
    "        # Process each text's entities\n",
    "        for text_idx, entities in enumerate(ner_results):\n",
    "            process_entities(\n",
    "                entities,\n",
    "                entity_sentiments,\n",
    "                sentiment_scores[text_idx],\n",
    "                dates[text_idx]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "\n",
    "    return entity_sentiments\n",
    "\n",
    "def process_entities(entities, entity_sentiments, sentiment_score, date):\n",
    "    \"\"\"Process entities from a single text and associate them with a date.\"\"\"\n",
    "    current_entity = \"\"\n",
    "    current_type = \"\"\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity']\n",
    "\n",
    "        if entity_type.startswith('B-'):\n",
    "            # Finalize the previous entity if it exists\n",
    "            if current_entity:\n",
    "                add_entity_with_date(\n",
    "                    current_entity.strip(),\n",
    "                    current_type,\n",
    "                    entity_sentiments,\n",
    "                    sentiment_score,\n",
    "                    date\n",
    "                )\n",
    "            # Start a new entity\n",
    "            current_entity = entity['word'].replace('##', '').strip()\n",
    "            current_type = entity_type[2:]  # Extract the type (e.g., 'ORG', 'PER')\n",
    "\n",
    "        elif entity_type.startswith('I-') and current_type == entity_type[2:]:\n",
    "            # Continue the current entity\n",
    "            current_entity += \" \" + entity['word'].replace('##', '').strip()\n",
    "\n",
    "        else:\n",
    "            # If it's 'O' or mismatched 'I-*', finalize the current entity\n",
    "            if current_entity:\n",
    "                add_entity_with_date(\n",
    "                    current_entity.strip(),\n",
    "                    current_type,\n",
    "                    entity_sentiments,\n",
    "                    sentiment_score,\n",
    "                    date\n",
    "                )\n",
    "                current_entity = \"\"\n",
    "                current_type = \"\"\n",
    "\n",
    "    # Add the last entity\n",
    "    if current_entity:\n",
    "        add_entity_with_date(\n",
    "            current_entity.strip(),\n",
    "            current_type,\n",
    "            entity_sentiments,\n",
    "            sentiment_score,\n",
    "            date\n",
    "        )\n",
    "\n",
    "def add_entity_with_date(entity, entity_type, entity_sentiments, sentiment_score, date):\n",
    "    \"\"\"Add an entity with its sentiment score and date to the entity_sentiments dictionary.\"\"\"\n",
    "    if entity_type in ['PER', 'ORG', 'MISC']:\n",
    "        mapped_type = {\n",
    "            'PER': 'People',\n",
    "            'ORG': 'Organizations',\n",
    "            'MISC': 'Technologies'\n",
    "        }[entity_type]\n",
    "        entity_sentiments[(mapped_type, entity)].append({'sentiment': sentiment_score, 'date': date})\n",
    "\n",
    "def analyze_entities(df, batch_size=32):\n",
    "    \"\"\"Process entity extraction and sentiment analysis on GPU\"\"\"\n",
    "    # Setup NER pipeline\n",
    "    ner_pipe = setup_pipelines(batch_size=batch_size)\n",
    "\n",
    "    # Initialize combined results\n",
    "    combined_sentiments = defaultdict(list)\n",
    "\n",
    "    # Process the data in batches\n",
    "    num_batches = math.ceil(len(df) / batch_size)\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_texts = df['text'][start_idx:end_idx].tolist()\n",
    "        batch_sentiments = df['sentiment_score'][start_idx:end_idx].tolist()\n",
    "        batch_dates = df['date'][start_idx:end_idx].tolist()\n",
    "\n",
    "        # Process batch\n",
    "        batch_results = process_batch(batch_texts, ner_pipe, batch_sentiments, batch_dates)\n",
    "\n",
    "        # Merge batch results into combined results\n",
    "        for (entity_type, entity), sentiments in batch_results.items():\n",
    "            combined_sentiments[(entity_type, entity)].extend(sentiments)\n",
    "\n",
    "    # Create summary dataframe\n",
    "    summary = []\n",
    "    for (entity_type, entity), sentiments in combined_sentiments.items():\n",
    "        avg_sentiment = np.mean([s['sentiment'] for s in sentiments])\n",
    "        mentions = len(sentiments)\n",
    "        sentiment_std = np.std([s['sentiment'] for s in sentiments]) if mentions > 1 else 0\n",
    "\n",
    "        summary.append({\n",
    "            'entity_type': entity_type,\n",
    "            'entity': entity,\n",
    "            'avg_sentiment': avg_sentiment,\n",
    "            'sentiment_std': sentiment_std,\n",
    "            'mentions': mentions,\n",
    "            'sentiment_label': 'Positive' if avg_sentiment > 0 else 'Negative',\n",
    "            'first_mention_date': min([s['date'] for s in sentiments]),\n",
    "            'last_mention_date': max([s['date'] for s in sentiments])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "def plot_timeline(summary_df, entity_type=None):\n",
    "    \"\"\"Plot a timeline to illustrate sentiment changes or entity mentions over time.\"\"\"\n",
    "    if entity_type:\n",
    "        data = summary_df[summary_df['entity_type'] == entity_type]\n",
    "    else:\n",
    "        data = summary_df\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        entity = row['entity']\n",
    "        dates = [s['date'] for s in combined_sentiments[(row['entity_type'], entity)]]\n",
    "        sentiments = [s['sentiment'] for s in combined_sentiments[(row['entity_type'], entity)]]\n",
    "\n",
    "        plt.plot(dates, sentiments, marker='o', label=entity)\n",
    "\n",
    "    plt.title(f\"Timeline for {entity_type if entity_type else 'All Entities'}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/adsp-nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "Error processing batch: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "batch_size = 32  # Adjust based on GPU memory\n",
    "\n",
    "# Process the data\n",
    "entity_summary = analyze_entities(df, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary.to_csv(\"entity_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary = pd.read_csv(\"entity_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>entity</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>sentiment_std</th>\n",
       "      <th>mentions</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>first_mention_date</th>\n",
       "      <th>last_mention_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>NASA Watch</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>2021-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>Daily News lette</td>\n",
       "      <td>0.931764</td>\n",
       "      <td>0.334446</td>\n",
       "      <td>135</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>2024-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>International Space Station</td>\n",
       "      <td>0.978600</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>14</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>2024-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>NASA</td>\n",
       "      <td>0.862361</td>\n",
       "      <td>0.444931</td>\n",
       "      <td>1097</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>2024-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Technologies</td>\n",
       "      <td>AI</td>\n",
       "      <td>0.855550</td>\n",
       "      <td>0.457312</td>\n",
       "      <td>208630</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583803</th>\n",
       "      <td>583803</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>K hara</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>2023-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583804</th>\n",
       "      <td>583804</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>Ke et mans ho op</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>2023-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583805</th>\n",
       "      <td>583805</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>N dak olo</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>2023-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583806</th>\n",
       "      <td>583806</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>Men E P Junior</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>2023-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583807</th>\n",
       "      <td>583807</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>Mat Navy</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>2023-11-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583808 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0    entity_type                       entity  avg_sentiment  \\\n",
       "0                0  Organizations                   NASA Watch       0.994900   \n",
       "1                1  Organizations             Daily News lette       0.931764   \n",
       "2                2  Organizations  International Space Station       0.978600   \n",
       "3                3  Organizations                         NASA       0.862361   \n",
       "4                4   Technologies                           AI       0.855550   \n",
       "...            ...            ...                          ...            ...   \n",
       "583803      583803  Organizations                       K hara       0.999300   \n",
       "583804      583804  Organizations             Ke et mans ho op       0.999300   \n",
       "583805      583805  Organizations                    N dak olo       0.999300   \n",
       "583806      583806  Organizations               Men E P Junior       0.999300   \n",
       "583807      583807  Organizations                     Mat Navy       0.999300   \n",
       "\n",
       "        sentiment_std  mentions sentiment_label first_mention_date  \\\n",
       "0            0.000000         1        Positive         2021-07-05   \n",
       "1            0.334446       135        Positive         2020-05-25   \n",
       "2            0.052882        14        Positive         2020-05-25   \n",
       "3            0.444931      1097        Positive         2020-01-10   \n",
       "4            0.457312    208630        Positive         2020-01-01   \n",
       "...               ...       ...             ...                ...   \n",
       "583803       0.000000         1        Positive         2023-11-07   \n",
       "583804       0.000000         1        Positive         2023-11-07   \n",
       "583805       0.000000         1        Positive         2023-11-07   \n",
       "583806       0.000000         1        Positive         2023-11-07   \n",
       "583807       0.000000         1        Positive         2023-11-07   \n",
       "\n",
       "       last_mention_date  \n",
       "0             2021-07-05  \n",
       "1             2024-07-12  \n",
       "2             2024-02-05  \n",
       "3             2024-11-05  \n",
       "4             2024-11-07  \n",
       "...                  ...  \n",
       "583803        2023-11-07  \n",
       "583804        2023-11-07  \n",
       "583805        2023-11-07  \n",
       "583806        2023-11-07  \n",
       "583807        2023-11-07  \n",
       "\n",
       "[583808 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>entity</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>sentiment_std</th>\n",
       "      <th>mentions</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>first_mention_date</th>\n",
       "      <th>last_mention_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, entity_type, entity, avg_sentiment, sentiment_std, mentions, sentiment_label, first_mention_date, last_mention_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_summary[entity_summary['entity'] == 'AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary = entity_summary[entity_summary['sentiment_label'] == \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== People Analysis ===\n",
      "\n",
      "Top 10 most mentioned entities:\n",
      "Empty DataFrame\n",
      "Columns: [entity, mentions, avg_sentiment, sentiment_label]\n",
      "Index: []\n",
      "\n",
      "=== Organizations Analysis ===\n",
      "\n",
      "Top 10 most mentioned entities:\n",
      "Empty DataFrame\n",
      "Columns: [entity, mentions, avg_sentiment, sentiment_label]\n",
      "Index: []\n",
      "\n",
      "=== Technologies Analysis ===\n",
      "\n",
      "Top 10 most mentioned entities:\n",
      "Empty DataFrame\n",
      "Columns: [entity, mentions, avg_sentiment, sentiment_label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Sort and display results\n",
    "for entity_type in ['People', 'Organizations', 'Technologies']:\n",
    "    print(f\"\\n=== {entity_type} Analysis ===\")\n",
    "    type_entities = entity_summary[entity_summary['entity_type'] == entity_type]\n",
    "    print(\"\\nTop 10 most mentioned entities:\")\n",
    "    print(type_entities.nlargest(50, 'mentions')[\n",
    "        ['entity', 'mentions', 'avg_sentiment', 'sentiment_label']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
